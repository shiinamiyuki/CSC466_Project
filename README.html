<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8"/>
	<link type="text/css" rel="stylesheet" href="css/github-markdown.css"/>
<link rel="stylesheet" href="css/github-markdown.css">
<style>
.markdown-body {
box-sizing: border-box;
min-width: 200px;
max-width: 980px;
margin: 0 auto;
padding: 45px;
}
</head>
<body>

<pre><code>@media (max-width: 767px) {
    .markdown-body {
        padding: 15px;
    }
}
</code></pre>

<p></style>
<article class="markdown-body"></p>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<div style="display:none">
<span class="math">\(\newcommand{\A}{\mat{A}}\)</span>
<span class="math">\(\newcommand{\B}{\mat{B}}\)</span>
<span class="math">\(\newcommand{\C}{\mat{C}}\)</span>
<span class="math">\(\newcommand{\D}{\mat{D}}\)</span>
<span class="math">\(\newcommand{\E}{\mat{E}}\)</span>
<span class="math">\(\newcommand{\F}{\mat{F}}\)</span>
<span class="math">\(\newcommand{\G}{\mat{G}}\)</span>
<span class="math">\(\newcommand{\H}{\mat{H}}\)</span>
<span class="math">\(\newcommand{\I}{\mat{I}}\)</span>
<span class="math">\(\newcommand{\J}{\mat{J}}\)</span>
<span class="math">\(\newcommand{\K}{\mat{K}}\)</span>
<span class="math">\(\newcommand{\L}{\mat{L}}\)</span>
<span class="math">\(\newcommand{\M}{\mat{M}}\)</span>
<span class="math">\(\newcommand{\N}{\mat{N}}\)</span>
<span class="math">\(\newcommand{\One}{\mathbf{1}}\)</span>
<span class="math">\(\newcommand{\P}{\mat{P}}\)</span>
<span class="math">\(\newcommand{\Q}{\mat{Q}}\)</span>
<span class="math">\(\newcommand{\Rot}{\mat{R}}\)</span>
<span class="math">\(\newcommand{\R}{\mathbb{R}}\)</span>
<span class="math">\(\newcommand{\S}{\mathcal{S}}\)</span>
<span class="math">\(\newcommand{\T}{\mat{T}}\)</span>
<span class="math">\(\newcommand{\U}{\mat{U}}\)</span>
<span class="math">\(\newcommand{\V}{\mat{V}}\)</span>
<span class="math">\(\newcommand{\W}{\mat{W}}\)</span>
<span class="math">\(\newcommand{\X}{\mat{X}}\)</span>
<span class="math">\(\newcommand{\Y}{\mat{Y}}\)</span>
<span class="math">\(\newcommand{\argmax}{\mathop{\text{argmax}}}\)</span>
<span class="math">\(\newcommand{\argmin}{\mathop{\text{argmin}}}\)</span>
<span class="math">\(\newcommand{\a}{\vec{a}}\)</span>
<span class="math">\(\newcommand{\b}{\vec{b}}\)</span>
<span class="math">\(\newcommand{\c}{\vec{c}}\)</span>
<span class="math">\(\newcommand{\d}{\vec{d}}\)</span>
<span class="math">\(\newcommand{\e}{\vec{e}}\)</span>
<span class="math">\(\newcommand{\f}{\vec{f}}\)</span>
<span class="math">\(\newcommand{\g}{\vec{g}}\)</span>
<span class="math">\(\newcommand{\mat}[1]{\mathbf{#1}}\)</span>
<span class="math">\(\newcommand{\min}{\mathop{\text{min}}}\)</span>
<span class="math">\(\newcommand{\m}{\vec{m}}\)</span>
<span class="math">\(\newcommand{\n}{\vec{n}}\)</span>
<span class="math">\(\newcommand{\p}{\vec{p}}\)</span>
<span class="math">\(\newcommand{\q}{\vec{q}}\)</span>
<span class="math">\(\newcommand{\r}{\vec{r}}\)</span>
<span class="math">\(\newcommand{\transpose}{{\mathsf T}}\)</span>
<span class="math">\(\newcommand{\tr}[1]{\mathop{\text{tr}}{\left(#1\right)}}\)</span>
<span class="math">\(\newcommand{\s}{\vec{s}}\)</span>
<span class="math">\(\newcommand{\t}{\vec{t}}\)</span>
<span class="math">\(\newcommand{\u}{\vec{u}}\)</span>
<span class="math">\(\newcommand{\vec}[1]{\mathbf{#1}}\)</span>
<span class="math">\(\newcommand{\x}{\vec{x}}\)</span>
<span class="math">\(\newcommand{\y}{\vec{y}}\)</span>
<span class="math">\(\newcommand{\z}{\vec{z}}\)</span>
<span class="math">\(\newcommand{\0}{\vec{0}}\)</span>
<span class="math">\(\renewcommand{\v}{\vec{v}}\)</span>
<!-- https://github.com/mathjax/MathJax/issues/1766 -->
<span class="math">\(\renewcommand{\hat}[1]{\widehat{#1}}\)</span>
</div>

<h1 id="computergraphics–kinematics">Computer Graphics – Kinematics</h1>

<blockquote>
<p><strong>To get started:</strong> Clone this repository using</p>

<pre><code>git clone --recursive http://github.com/alecjacobson/computer-graphics-kinematics.git
</code></pre>
</blockquote>

<h2 id="background">Background</h2>

<figure>
<img src="images/robot-arm.gif" alt="" />
</figure>

<h3 id="readchapter15.1-15.5offundamentalsofcomputergraphics4thedition.">Read Chapter 15.1&#8211;15.5 of <em>Fundamentals of Computer Graphics (4th Edition)</em>.</h3>

<h3 id="readchapter16.1-16.4offundamentalsofcomputergraphics4thedition.">Read Chapter 16.1&#8211;16.4 of <em>Fundamentals of Computer Graphics (4th Edition)</em>.</h3>

<h3 id="skeleton">Skeleton</h3>

<p>In this assignment we&#8217;ll consider animating shapes <em>rigged</em> to an internal
skeleton. The skeleton is a <a href="https://en.wikipedia.org/wiki/Interface_metaphor">(graphical) user interface (UI)
<em>metaphor</em></a>. A skeleton is a
<a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">tree</a> of rigid bones, not
unlike the <a href="https://en.wikipedia.org/wiki/Bone">anatomical bones</a> in a human or
animal.</p>

<p>Each &#8220;bone&#8221; in the skeleton is really a UI widget for visualizing and
controlling a 3D <a href="https://en.wikipedia.org/wiki/Rigid_transformation">rigid
transformation</a>. A common
visualization of 3D bone in computer graphics is a long, pointed <a href="https://en.wikipedia.org/wiki/Pyramid_(geometry)">pyramid
shape</a>. This reveals the
twisting rotation as well as the tree hierarchy: the bone points toward its
children.</p>

<figure>
<img src="images/robot-arm-wireframe.gif" alt="" />
</figure>

<p><em>Unlike</em> anatomy where the brain triggers muscles to flex and pull the passive
bones around, the bones of a skeleton rig will define the pose of a shape.</p>

<p>For each bone, we will consider <em><strong>three</strong></em> &#8220;states&#8221;.</p>

<h4 id="1.canonicalbone">1. Canonical Bone</h4>

<p>The &#8220;Canonical Bone&#8221; of length <span class="math">\(\ell\)</span> lies along the <span class="math">\(x\)</span>-axis with its &#8220;tail&#8221; at
the origin <span class="math">\((0,0,0)\)</span>, its &#8220;tip&#8221; at <span class="math">\((\ell,0,0)\)</span>. </p>

<figure>
<img src="images/canonical-bone.png" alt="" />
</figure>

<p>The bone is endowed with an
orientation or <a href="https://en.wikipedia.org/wiki/Moving_frame">frame</a>. This helps
define a canonical <em>twisting</em> direction. We will define twisting as rotating about the <span class="math">\(x\)</span>-axis in the canonical frame. </p>

<figure>
<img src="images/canonical-twisting.gif" alt="" />
</figure>

<p>For example, in this assignment, <em>bending</em> is accomplished by rotating about the
<span class="math">\(z\)</span>-axis in the canonical frame.</p>

<figure>
<img src="images/canonical-bending.gif" alt="" />
</figure>

<p>Composing a twist, bend and
another twist spans all possible 3D rotations. </p>

<figure>
<img src="images/canonical-rotating.gif" alt="" />
</figure>

<p>We call the three angles composed
into a rotation this way, <a href="https://en.wikipedia.org/wiki/Euler_angles">Euler angles</a> (not to be confused with
the <a href="https://en.wiktionary.org/wiki/homophonous">homophonous</a> <a href="https://upload.wikimedia.org/wikipedia/commons/9/97/Odessa_TX_Oil_Well_with_Lufkin_320D_pumping_unit.gif">Oiler
angles</a>).</p>

<h4 id="2.restbone">2. Rest Bone</h4>

<p>To assemble a skeleton inside our shape will we map each bone from its
<a href="#1.canonicalbone">canonical bone</a> to its position <em>and orientation</em> in the
undeformed model. Maintaining the rigidity of the bone, this means for each bone
there&#8217;s a rigid transformation <span class="math">\(\hat{\T} = (\hat{\Rot} \quad \hat{\t} ) ∈
\R^{3×4}\)</span> that places its tail and tip to the desired positions in the model. </p>

<figure>
<img src="images/rest-bone.png" alt="" />
</figure>

<p>We
use the convention that the &#8220;canonical tail&#8221; (the origin <span class="math">\((0,0,0)\)</span>) is mapped to
the &#8220;rest tail&#8221; inside the model. This means that the <em>translation</em> part of the
matrix <span class="math">\(\hat{\T}\)</span> is simply the tail position, <span class="math">\(\hat{\s}∈\R^3\)</span>:
$$
\hat{\s} = \hat{\T} \left(\begin{array}{c}0\\0\\0\\1\end{array}\right) =
\hat{\Rot} \left(\begin{array}{c}0\\0\\0\end{array}\right) + \hat{\t} 1 = \hat{\t}.
$$</p>

<p>The bone&#8217;s
rotation is chosen so that the &#8220;canonical tip&#8221; <span class="math">\((\ell,0,0)\)</span> maps to the &#8220;rest
tip&#8221; <span class="math">\(\hat{\d} ∈ \R³\)</span>:
$$
\hat{\d} =
\hat{\T} \left(\begin{array}{c}\ell\\0\\0\\1\end{array}\right) =
\hat{\Rot}\left(\begin{array}{c}\ell\\0\\0\end{array}\right) + \hat{\t}.
$$</p>

<p>Typically the &#8220;rest tail&#8221; of is coincident with the &#8220;rest tip&#8221; of its
parent (if it exists):
$$
\hat{\d}_{p} = \hat{\s}.
$$</p>

<p>This still leaves any amount of <em>twisting</em> of the bone. In the canonical frame,
we can think of this as pre-twisting the bone along the canonical <span class="math">\(x\)</span>-axis.
Clearly, twisting does not effect the ability to map the tail and tip to the
correct position. This twist is <em>chosen</em> so that canonical bending aligns with a
meaningful direction. For example, we may twist a <a href="https://en.wikipedia.org/wiki/Tibia">tibia
(shinbone)</a> bone so that canonical bending
around the <span class="math">\(z\)</span>-axis means bending at the
<a href="https://en.wikipedia.org/wiki/Knee">knee</a>.</p>

<p>Each rest transformation <span class="math">\(\hat{\T}\)</span> <em>places</em> its corresponding bone inside the
undeformed shape. The rest transformations do not measure any deformation of the
shape from its original position. Thus, the <em>pose</em> of each bone will be measured
<em>relative</em> to the &#8220;rest bone&#8221;.</p>

<h4 id="3.posebone">3. Pose Bone</h4>

<p>The final state to consider is when a bone is <em>posed</em>. That is, mapped to a new
position and orientation from its rest state. </p>

<p>In general, each rest bone undergoes a rigid transformation <span class="math">\(\T ∈ \R^{3×4}\)</span>,
composed of a rotation <span class="math">\(\Rot ∈ \R^{3×3}\)</span> and a translation <span class="math">\(\t∈\R^{3}\)</span>, mapping each of its
rest points <span class="math">\(\hat{x} ∈ \R³\)</span> to its corresponding posed postion <span class="math">\(\x ∈ \R³\)</span>:</p>

<p>$$
\x = \T \hat{\x}.
$$</p>

<figure>
<img src="images/beast-pose-bone.gif" alt="" />
</figure>

<p><span class="math">\(\T\)</span> is expressed as a <em>global</em> mapping of any point in the rest reference frame
to its pose position. This makes it convenient for <a href="#linearblendskinning">blending transformations
(see below)</a>, but it&#8217;s not so obvious how to pick coherent
values for <span class="math">\(\T\)</span>. In particular, we would like each bone to rotate about its
parent&#8217;s tip, but this position is determined by the parent&#8217;s pose
transformation <span class="math">\(\T_p\)</span>, which in turn should rotate about the grandparent&#8217;s tip
and so on.</p>

<h3 id="forwardkinematics">Forward Kinematics</h3>

<p>One way to determine the rigid pose transformations <span class="math">\(\T_i ∈ \R^{3×4}\)</span> for each
bone <span class="math">\(i\)</span> in a skeleton is to aggregate <em>relative rotations</em> <span class="math">\(\overline{\Rot}_i ∈
\R^{3×3}\)</span> between a bone <span class="math">\(i\)</span> and its parent bone <span class="math">\(p_i\)</span> in the skeletal tree.
The final transformation at some bone <span class="math">\(i\)</span> deep in the skeletal tree is computed
via a recursive equation.</p>

<p>For each bone, (reading the effect of transformations <em>right to left</em>) we first
<em>undo</em> the map from canonical to rest (i.e., via inverting <span class="math">\(\hat{\T}_i\)</span>), then
rotate by our relative rotation <span class="math">\(\overline{\Rot}_i\)</span>, then map back to rest (via
<span class="math">\(\hat{T}_i\)</span>). With our relative transformation accomplished, we continue <em>up the
tree</em> <a href="https://en.wikipedia.org/wiki/Recursion_(computer_science)">recursively</a>
applying our parent&#8217;s relative transformation, and our grandparent&#8217;s and so on:
$$
\T_i = \T_{p_i}
\left(\begin{array}{c} \hat{\T}_i \\ 0\ 0\ 0 \ 1\end{array}\right)
\left(\begin{array}{cccc} \overline{\Rot}_i &amp; \begin{array}{c}0\\0\\0\\\end{array} \\ 0\ 0\ 0 &amp; 1\end{array}\right)
\left(\begin{array}{c} \hat{\T}_i \\ 0\ 0\ 0 \ 1\end{array}\right)^{&#8211;1}
$$</p>

<blockquote>
<p><strong>Question:</strong> Does using relative rotations ensure that bone tails stay
coincident with parent tips?</p>

<p><strong>Hint:</strong> What do you get if you multiply <span class="math">\(\T_i\)</span> and <span class="math">\(\hat{\s}_i\)</span>?</p>
</blockquote>

<p>As a base case, the <em>root</em> transformation can be defined to be the identity (no
transformation) or the rigid transformation placing the object/character
generally into a larger scene.</p>

<p>This has the great advantage that if the entire model is rotated or translated
at the root, the relative transformations still apply correctly. This property
holds locally, too. If bone <span class="math">\(i\)</span> controls the <a href="https://en.wikipedia.org/wiki/Tibia">tibia
(shinbone)</a> and <span class="math">\(\Rot_i\)</span> applies a bend at
the knee, then twisting and bending at the parent hip bone will naturally
<em>compose</em> with the knee bend.</p>

<p>It is convenient to express the relative rotations of each bone in the
<a href="#1.canonicalbone">canonical frame</a>. We can utilize canonical twist-bend-twist
rotations (three <a href="https://en.wikipedia.org/wiki/Euler_angles">Euler angles</a>, <span class="math">\(Θ₁,θ₂,θ₃\)</span>). Each bone&#8217;s rotation
is conducted in its canonical frame and then <em>brought</em> through the rest frame
through a change of coordinates:</p>

<p>$$
\T_i
=
\T_{p_i}
\hat{\T}_i
\left(\begin{array}{cccc} \Rot_x(θ_{i3}) &amp; \begin{array}{c}0\\0\\0\\\end{array} \\ 0\ 0\ 0 &amp; 1\end{array}\right)
\left(\begin{array}{cccc} \Rot_z(θ_{i2}) &amp; \begin{array}{c}0\\0\\0\\\end{array} \\ 0\ 0\ 0 &amp; 1\end{array}\right)
\left(\begin{array}{cccc} \Rot_x(θ_{i1}) &amp; \begin{array}{c}0\\0\\0\\\end{array} \\ 0\ 0\ 0 &amp; 1\end{array}\right)
\left.\hat{\T}_i\right.^{&#8211;1}
$$
where the matrix <span class="math">\(\Rot_w(φ) ∈ \R^{3×3}\)</span> is the rotation by <span class="math">\(φ\)</span> degrees around
the <span class="math">\(w\)</span>-axis.</p>

<p>When implementing a skeleton, it is tempting to use a traditional <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">tree data
structure</a> where each node
(bone) contains a list of pointers to its children (other bones). However, by
the <em>right-to-left</em> reading of the forward kinematics formulae above, it is more
convenient to use a data structure where each node (bone) has a pointer to its
(unique) parent (other bone). This is ridiculously named a <a href="https://en.wikipedia.org/wiki/Parent_pointer_tree">Spaghetti
Stack</a>.</p>

<blockquote>
<p><strong>Question:</strong> What abstract data-structure is good for ensuring a parent&#8217;s transformation <span class="math">\(\T_{p_i}\)</span>
are computed before its child&#8217;s <span class="math">\(\T_i\)</span>?</p>

<p><strong>Hint:</strong> 🥞</p>
</blockquote>

<h3 id="keyframeanimation">Keyframe animation</h3>

<p>To create a long animation, specifying three Euler angles for every bone for
every frame manually would be too difficult. The standard way to determine the
relative bone transformations for each frame is to
<a href="https://en.wikipedia.org/wiki/Interpolation">interpolate</a> values specified at a
small number of &#8220;key&#8221; times during the animation. <a href="https://en.wikipedia.org/wiki/Linear_interpolation">Linear
interpolation</a> will lead to
a choppy, robotic animation (try this first!). Instead <a href="https://en.wikipedia.org/wiki/Cubic_Hermite_spline#Catmull–Rom_spline">Catmull-Rom
interpolation</a>
will produce a smooth animation. Fancier interpolation such as the
<a href="https://en.wikipedia.org/wiki/Kochanek–Bartels_spline">Kochanek-Bartels method</a>
(called TCB in <a href="https://www.cs.cornell.edu/~srm/fcg4/">the book</a>) allow better
control of <a href="https://en.wikipedia.org/wiki/12_basic_principles_of_animation#Slow_In_and_Slow_Out">easing between key
poses</a>.</p>

<figure>
<img src="images/hand-animation-sped-up.gif" alt="" />
</figure>

<p>In this assignment, we will interpolate Euler angles directly. This works well
when only a single angle is changing at a time. However, <a href="https://en.wikipedia.org/wiki/Gimbal_lock">Euler angles do not
provide easy movement in every rotational
direction</a>. Euler angles model
rotations as <em>twist-bend-twist</em>. For our canonical bones, bending around the
<span class="math">\(z\)</span>-axis is easy, but bending around the <span class="math">\(y\)</span>-axis requires first twisting by
<span class="math">\(90°\)</span> and then &#8220;un&#8221;-twisting by <span class="math">\(-90°\)</span> after bending.</p>

<p>So, for more complex interpolation of rotations, a different representation such
as <a href="https://en.wikipedia.org/wiki/Slerp">unit quaternions</a> would be needed. This is
outside the scope of this assignment.</p>

<h3 id="inversekinematics">Inverse Kinematics</h3>

<p>In the <a href="#forwardkinematics">forward kinematics</a> model, the final position of the
tip of a finger is determined by setting (manually or via interpolation) the
relative transformations of each joint in the finger, the hand, the elbow, the
shoulder, &#8230; This
<a href="https://en.wikipedia.org/wiki/Direct_manipulation_interface">indirect</a> control
makes it difficult to achieve basic poses. Instead, we can treat the problem of
setting relative rotations of internal bones (shoulder, elbow, hand, &#8230;) as an
optimization problem where we try to minimize the distance between the tip of
the finger and where we want it to be.</p>

<p>Stated mathematically, for a skeleton with <span class="math">\(m\)</span> bones, if we create a vector
<span class="math">\(\a ∈ \R^{3m}\)</span> stacking all the Euler angles of each bone vertically:
$$\a = \left(\begin{array}{c}
θ_{11} \\
θ_{12} \\
θ_{13} \\
θ_{21} \\
θ_{22} \\
θ_{23} \\
\vdots \\
θ_{m1} \\
θ_{m2} \\
θ_{m3}
\end{array}\right)
$$</p>

<p>then we can ask for the best vector of angles <span class="math">\(θ\)</span>. Best-ness must be quantified
by an cost/energy/obective-function <span class="math">\(E\)</span>. This energy is typically first written
with respect to the (global, non-relative) pose positions of certains bones
<span class="math">\(\x_b ∈ \R³\)</span> (often the &#8220;tip&#8221; of a
<a href="https://en.wikipedia.org/wiki/Tree_(data_structure)#Terminology_used_in_trees">leaf</a>
bone of the skeletal tree, called an <a href="https://en.wikipedia.org/wiki/Robot_end_effector">end
effector</a>). For example, we
then we could design our energy to measure the squared distance between the pose
tip <span class="math">\(\x_b\)</span> of some bone <span class="math">\(b\)</span> and a desired goal location <span class="math">\(\q∈\R³\)</span>:</p>

<p>$$
E(\x_b) = ‖\x_b - \q‖².
$$</p>

<p>Using forward kinematics, we can express <span class="math">\(\x_b\)</span> and in turn <span class="math">\(E\)</span> with respect to
relative rotations: </p>

<p>$$
\x_b(\a) = \T_b \hat{\d}_b
$$
where <span class="math">\(\T_b\)</span> depends on <span class="math">\(θ_{b1},θ_{b2},θ_{b2}\)</span> and <span class="math">\(\T_{p_b}\)</span> which depends on
<span class="math">\(θ_{p_b1},θ_{p_b2},θ_{p_b2}\)</span>. In this way our energy can be written as a
function of <span class="math">\(\a\)</span>:</p>

<p>$$
E(\x_b(\a)) = ‖\x_b(\a) - \q‖².
$$</p>

<p>We can design arbitrarily complex energies to satisfy our interaction needs. In
this assignment, we consider that there is a list of constrained end effectors
<span class="math">\(b = \{b₁,b₂,…,b_k\}\)</span> and our objective is that all selected end effectors <span class="math">\(b_i\)</span>
go to their prescribed locations (provided by the mouse-drag UI).
using the simple squared distance measure above.</p>

<p>So, over all choices of <span class="math">\(\a\)</span> we&#8217;d like to optimize:</p>

<p>$$
\min_{\a} \quad
\underbrace{
∑\limits_{i=1}^k ‖\x_{b_i}(\a) - \hat{\x}_{b_i}‖²
}_{E(\x_b (\a))}
$$</p>
<!--
Our energy will have two terms.

First, for a certain constrained end effector $b_j$ we ask that it lies as close
as possible to the [viewing
ray](https://en.wikipedia.org/wiki/Ray_tracing_(graphics)) through the user's
[mouse pointer](https://en.wikipedia.org/wiki/Pointer_(user_interface)). 
We can measure the distance between some point $\x$ and the line passing
through the camera/eye location $\e$ and the mouse location _unprojected_ onto
its 3D position on the screen placed in the scene at $\m$ using:
$$
E_\text{mouse}(\x) = \left|\left| (\x - \m) - \left((\x - \m)⋅\frac{\e-\m}{‖\e-\m‖}\right)\frac{\e-\m}{‖\e-\m‖}\right|\right|^2.
$$
This formula can be simplified to a much simpler expression since $\m$ and $\e$
do not depend on $\x$ (left to the reader/implementor).


> **Question:** How would you _alternatively_ write this term by measuring
> distances in image space?
>
> **Hint:** 📽
>

Second, for all of the other end effectors $b_i \left|\right. i≠j$, we will
constrain their positions to their rest locations $\hat{\x}_{b_i}$, using the
simple squared distance measure above.

The goal of inverse kinematics is to minimize the sum of these energies over all
choices of $\a$:

$$
\min_{\a} \quad
\underbrace{
E_{\text{mouse}}(\x_{b_j}(\a)) + 
∑\limits_{i≠j} ‖\x_{b_i}(\a) - \hat{\x}_{b_i}‖²
}_{E(\x_b (\a))}
$$
-->

<p>We will further constrain our problem by imposing
<a href="https://en.wikipedia.org/wiki/Constrained_optimization#Inequality_constraints">upper and lower bounds</a>
on our angles <span class="math">\(\a\)</span>. These correspond to joint limits. For example, the joint
limits of a hinge or elbow type joint may look like:
$$
0° ≤ θ₁ ≤ 0°, \quad 0° ≤ θ₂ ≤ 170°, \quad 0° ≤ θ₃ ≤ 0°.
$$
These would ensure that our joint cannot twist, and can only bend in one direction.</p>

<p>So our full optimization problem becomes </p>

<p>$$
\min_{\a^{\text{min}} ≤ \a ≤
\a^{\text{max}}}
\quad E(\x_b(\a))
$$
where <span class="math">\(\a^{\text{min}}/\a^{\text{max}}\)</span> stack lower/upper bounds correspondingly to <span class="math">\(\a\)</span>.</p>

<figure>
<img src="images/ikea-lamp-ik.gif" alt="" />
</figure>

<p>This type of minimization is non-trivial. Our energy is a quadratic <a href="https://en.wikipedia.org/wiki/Linear_least_squares">sum of
squares</a> in <span class="math">\(\x_b\)</span>, but
<span class="math">\(\x_b\)</span> is a non-linear function of <span class="math">\(\a\)</span>. In turn, this means to minimize <span class="math">\(E\)</span> as
a function of <span class="math">\(\a\)</span> we must solve a <a href="https://en.wikipedia.org/wiki/Non-linear_least_squares">non-linear least
squares</a> problem. </p>

<h4 id="projectedgradientdescent">Projected Gradient Descent</h4>

<p>We&#8217;re faced with a bound constrained non-linear optimization problem. To solve
it, we will construct an initial guess and then iteratively improve the guess by
moving in a direction that decreases <span class="math">\(E\)</span>, after each step <em>snap</em> or project the
guess to stay within the bounds if necessary. This algorithm is called
<em>projected gradient descent</em>.</p>

<p>The idea behind <a href="https://en.wikipedia.org/wiki/Gradient_descent"><em>gradient
descent</em></a> is intuitive: if you
want to get to the bottom of a canyon, look at the ground and walk in the
direction that goes downhill.</p>

<p>So, we iteratively take a step in the <em>negative</em> gradient direction of our
objective function <span class="math">\(E(\x(\a))\)</span>:</p>

<p>$$
\a ← \a - σ \left(\frac{dE(\x(\a))}{d\a}\right)^T
$$</p>

<p>Applying the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a>, this
iteration becomes</p>

<p>$$
\a ← \a - σ \left(\frac{d\x(\a)}{d\a}\right)^T\left(\frac{dE(\x)}{d\x}\right)
$$</p>

<p>where <span class="math">\(\frac{dE}{d\a} ∈ \R^{|\a|}\)</span>,
<span class="math">\(\frac{dE}{d\x} ∈ \R^{|\x|}\)</span>, and <span class="math">\(\frac{d\x}{d\a} ∈ \R^{|\x| × |\a|}\)</span></p>

<p>The change in tip positions <span class="math">\(\x\)</span> with respect to joint angles <span class="math">\(\a\)</span> does not
depend on the choice of energy <span class="math">\(E\)</span>. We call this matrix of changes the kinematic
<a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a>, <span class="math">\(\J ∈
\R^{|\x| × |\a|}\)</span>:</p>

<p>$$
\J = \frac{d\x}{d\a}.
$$</p>

<p>Written in terms of <span class="math">\(\J\)</span> our step becomes,</p>

<p>$$
\a ← \a - σ \J^\transpose\left(\frac{dE(\x)}{d\x}\right)
$$</p>

<blockquote>
<p><strong>Question:</strong> Can we take an arbitrarily large step <span class="math">\(σ>>0\)</span>?</p>

<p><strong>Hint:</strong> What if we just need to change <span class="math">\(\a\)</span> by a small, non-zero amount?
What would chooing <span class="math">\(σ=1,000,000\)</span> do to <span class="math">\(\a\)</span>? What would that in turn do to
<span class="math">\(E(\x(\a))\)</span>?</p>
</blockquote>

<p>For sufficiently small <span class="math">\(σ\)</span>, each step will decrease the objective energy <span class="math">\(E\)</span>.</p>

<p>If the gradient of <span class="math">\(E\)</span> becomes zero, then we&#8217;re at a <a href="https://en.wikipedia.org/wiki/Stationary_point">stationary
point</a> and likely at a minimum.</p>

<p>To ensure that our bounds are obeyed, after each step we need to <em>project</em> onto
our constraints by snapping each value to its respective bound if necessary:</p>

<p>$$
\a_i ← \max[\a^\text{min}_i, \min[\a^\text{max}_i,\a_i]].
$$</p>

<p>We&#8217;ll refer to this as a projection filter acting on the entire vector <span class="math">\(\a\)</span>:</p>

<p>$$
\a ← \text{proj}(\a).
$$</p>

<blockquote>
<h4 id="newtonsmethod">Newton&#8217;s method</h4>

<p>The local gradient of a function can be very different from the &#8220;best&#8221; descent
direction. The choice of <span class="math">\(σ\)</span> reflects how much we &#8220;trust&#8221; this direction.
Unfortunately, if <span class="math">\(σ\)</span> is too large our iterations may diverge. If <span class="math">\(σ\)</span> is too
small, we will have to do many iterations.</p>

<p>In order to find a <em>better</em> descent direction, let&#8217;s assume we knew <em>more</em> about
<span class="math">\(E\)</span>. That is, suppose we also knew its second derivatives: <span class="math">\(\frac{d²E}{d\x²}\)</span>. </p>

<p>Given an initial guess <span class="math">\(\x⁰\)</span> we&#8217;re looking to find a change <span class="math">\(∆\x\)</span> so that <span class="math">\(E(\x⁰
+ ∆\x)\)</span> is a stationary point.</p>

<p>Starting with our equilibrium equation,
$$
\frac{dE(\x)}{d\x} = \0
$$</p>

<p>we substitute in <span class="math">\(x = \x⁰ + ∆\x\)</span></p>

<p>$$
\frac{dE(\x⁰+∆\x)}{d∆\x} = \0
$$</p>

<p>Plugging in a <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor series</a>
expansion</p>

<p>$$
E(\x⁰+∆\x) \approx E(\x⁰) + \frac{d E(\x⁰+∆\x)}{d\x} ∆\x +
\frac{d²E(\x⁰+∆\x)}{d\x²}\frac{(∆\x)²}{2} + …$$</p>

<p>and dropping higher order terms (<span class="math">\(…\)</span>), we get:</p>

<p>$$
\frac{d}{d∆\x}(E(\x⁰) + \frac{d E(\x⁰+∆\x)}{d\x} ∆\x + \underbrace{\frac{d²E(\x⁰+∆\x)}{d\x²}}_\H\frac{(∆\x)²}{2}) = \0,
$$
where we call <span class="math">\(\H ∈ \R^{|x| × |x|}\)</span> the <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian
matrix</a>. </p>

<p>Applying the differentiation by <span class="math">\(∆\x\)</span> we get a system of equations:</p>

<p>$$
\frac{d E(\x⁰+∆\x)}{d\x} + \frac{d²E(\x⁰+∆\x)}{d\x²} ∆\x = \0.
$$
Solving for the change <span class="math">\(∆x\)</span> we get:
$$
∆x = -\left.\H\right.^{&#8211;1} \frac{d E(\x⁰+∆\x)}{d\x}.
$$</p>

<p>So a <em>raw</em> Newton&#8217;s method update would be:</p>

<p>$$
\x ← \x - \left.\H\right.^{&#8211;1} \frac{d E(\x⁰+∆\x)}{d\x}.
$$</p>

<p>If our Taylor series approximation was perfect (no high order terms in <span class="math">\(…\)</span>; in
otherwords <span class="math">\(E\)</span> was quadratic), then Newton&#8217;s method would be perfect: a single
update immediately takes us to the minimum.</p>

<p>Newton&#8217;s method is problematic for a number of reasons.</p>

<ol>
<li>We built our step purely based on the equations for a stationary point.
Nothing says we won&#8217;t get sent toward a maximum or saddle-point.</li>
<li><span class="math">\(\H\)</span> is often difficult or expensive to compute.</li>
<li><span class="math">\(\H\)</span> may be singular.</li>
<li>Inverting <span class="math">\(\H\)</span> (even if possible) is often slow.</li>
<li>Our system is built off a local approximation of <span class="math">\(E\)</span> so the descent
direction may <em>still</em> point in the wrong direction.</li>
</ol>

<p>Since we&#8217;re approximating <span class="math">\(E\)</span> at every iteration anyway, we&#8217;ll skirt many of
these issues by considering various approximations of the Hessian matrix <span class="math">\(\H\)</span>.
We&#8217;ll never actually compute <span class="math">\(\H\)</span>.</p>

<h4 id="gradientdescentrevisited">Gradient Descent <em>Revisited</em></h4>

<p>The simplest approximation of <span class="math">\(\H\)</span> is the identity matrix <span class="math">\(\I\)</span>. Plugging this
into our truncated Taylor series expansion above, our approximation would read:</p>

<p>$$
E(\x⁰) + \frac{d E(\x⁰+∆\x)}{d\x} ∆\x + \I \frac{(∆\x)²}{2}.
$$</p>

<p>and our step reduces to good ol&#8217; gradient descent:</p>

<p>$$
\x ← \x - \frac{d E(\x⁰+∆\x)}{d\x}.
$$</p>

<h4 id="gauss-newton">Gauss-Newton</h4>

<p>Given that we have already computed first derivatives in the Jacobian <span class="math">\(\J
=\frac{d\x(\a)}{d\a}\)</span>, an even better approximation for Hessian <span class="math">\(\H\)</span> than the
identity <span class="math">\(\I\)</span> would be to use <span class="math">\(\J^\transpose J\)</span>. The resulting update becomes:</p>

<p>$$
\a ← \a + (\left.\J\right.^\transpose\J)^{&#8211;1} \left.\J\right.^\transpose \frac{dE(\x)}{d\x}
$$</p>

<p>Unlike <span class="math">\(\H\)</span>, <span class="math">\(\J^\transpose\J\)</span> is easy to compute if we&#8217;re already computing
<span class="math">\(\J\)</span>. It is guaranteed to be <a href="https://en.wikipedia.org/wiki/Positive-definite_matrix">positive
semi-definite</a> and it
is possible to invert or reliably
<a href="https://en.wikipedia.org/wiki/Moore–Penrose_inverse">pseudo-invert</a> (<span class="math">\(\J^+\)</span>
acting in place of <span class="math">\((\left.\J\right.^\transpose\J)^{-1}
\left.\J\right.^\transpose\)</span>).</p>

<p>The descent directions are often significantly better than gradient descent.
As a result this method, called Gauss-Newton, requires many fewer iterations
to converge.</p>

<p>It still may try to descend in bad directions. In particular, for inverse
kinematics, this Gauss-Newton method performs poorly if the desired positions
are not reachable: over extending an arm. First the solution locks in place
and then diverges. This happens when our Hessian approximation
<span class="math">\(\J^\transpose\J\)</span> starts misbehaving.</p>

<p>A good fix is to blend between the gradient descent and Gauss-Newton search
directions. That is blend between <span class="math">\(\I\)</span> and <span class="math">\(\J^\transpose\J\)</span>. This is called
the <a href="https://en.wikipedia.org/wiki/Levenberg–Marquardt_algorithm">Levenberg-Marquadt
algorithm</a>.</p>
</blockquote>

<h4 id="finitedifferencing">Finite Differencing</h4>

<p>But how do we compute the kinematic Jacobian <span class="math">\(\J\)</span>? Since each entry in <span class="math">\(\x\)</span> is
the result of forward kinematics and not just a math expression, it&#8217;s not
immediately obvious how to determine its derivatives. However, a derivative is
nothing more than the limit of a small change output divided by a small change
in the input:</p>

<p>$$
\J_{i,j} = \lim_{h → 0} \frac{\x_i(\a+h δ_j) - \x_i(\a)}{h},
$$
where <span class="math">\(δ_j ∈ \R^{|a|}\)</span> is a vector of zeros except a 1 at location <span class="math">\(j\)</span>.</p>

<p>We can numerically approximate this limit by fixing <span class="math">\(h\)</span> to a small value (e.g.,
<span class="math">\(h=10^{-7}\)</span>). This is called the <a href="https://en.wikipedia.org/wiki/Finite_difference">finite
difference</a> approximation:
$$
\J_{i,j} \approx \frac{\x_i(\a+h δ_j) - \x_i(\a)}{h}.
$$</p>

<p>For inverse kinematics, we will need to compute <span class="math">\(\x_i(\a+h δ_j)\)</span> once for each
Euler angle of each bone <span class="math">\(j\)</span>. This requires <span class="math">\(3m\)</span> calls to our forward kinematics
function (each with a slightly different input), which is in turn <span class="math">\(O(m)\)</span>. This
makes the total cost <span class="math">\(O(m²)\)</span> to fill in our <span class="math">\(\J\)</span> matrix.</p>

<blockquote>
<h4 id="automaticdifferentiation">Automatic Differentiation</h4>

<p>Forward differencing requires <span class="math">\(O(m)\)</span> evaluations but doesn&#8217;t require us to
change our code for function evaluation <em>at all</em>: we just evaluate it. If
we&#8217;re willing to sprinkle some special types on top of our code (but otherwise
leave in all the sub-routine calls, if statements, for loops, etc.), we could
use <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic
differentiation</a> to
compute <span class="math">\(\J\)</span>.</p>

<p>The are two extremes when it comes to autodiff: forward mode and backward
mode. </p>

<p><strong>Forward mode</strong> works like finite differencing, except the perturbation to
the differentiation variable is symbolic and derivatives are tracked through
each basic operation (<code>+</code>,<code>-</code>,<code>sin</code>,etc.): the total computational cost to
construct <span class="math">\(\J\)</span> is again <span class="math">\(O(m²)\)</span>.</p>

<p><strong>Backward mode</strong> works by pushing each function call and basic operation onto
a list. Derivatives for all variables are then computed as we pop backward
through the evaluation: identical to how we read right-to-left on our
recursive kinematics formula. This means we compute derivatives with respect
to all variables <span class="math">\(\a\)</span> in a single <em>backwards</em> evaluation. The total cost is
only <span class="math">\(O(m)\)</span> to fill <span class="math">\(\J\)</span>. </p>
</blockquote>

<h4 id="linesearch">Line Search</h4>

<p>Whether we&#8217;re using gradient descent, Newton&#8217;s method or Gauss-Newton, we a
generally <em>attempting</em> improving our guess by iteratively moving in a descent
direction <span class="math">\(∆\a\)</span>, followed by projecting onto constraints:</p>

<p>$$
\a ← \text{proj}(\a + ∆\a).
$$</p>

<p>Despite our best efforts, this step is not guaranteed to actually decrease
our energy <span class="math">\(E\)</span>. We can think of the descent <em>direction</em> <span class="math">\(∆\a\)</span> as defining a line (or really
a <em>ray</em>) and we&#8217;d like to find a positive amount <span class="math">\(σ\)</span> to move along this line that actually
does decrease the energy:</p>

<p>$$
E(\text{proj}(\a + σ ∆\a)) &lt; E(\a).
$$</p>

<p>While there exists an optimal step <span class="math">\(σ\)</span>, we don&#8217;t want to spend too long finding
it as we would be better off spending our computational efforts improving the
descent <em>direction</em> for the next step. So, starting with a large value <span class="math">\(σ\)</span>
(e.g., 10,000), we decrease <span class="math">\(σ\)</span> by a constant factor (e.g., <span class="math">\(½\)</span>) until our
inequality passes.</p>

<p>Depending on the configuration, it may or may not be possible to exactly satisfy
the constraints (i.e., <span class="math">\(E = 0\)</span>). But after many iterations, the solution should
converge to a <a href="https://en.wikipedia.org/wiki/Maxima_and_minima">local minimum</a>
(i.e., <span class="math">\(E>0\)</span>, but <span class="math">\(dE/d\a = 0\)</span>). In our assignment, a thin line will appear if
the user-given constraint is not coincident with the end-effector tip position.</p>

<figure>
<img src="images/knight-dab.gif" alt="" />
</figure>

<h3 id="linearblendskinning">Linear Blend Skinning</h3>

<p>So far we have only discussed bones floating and moving around in space.
Ultimately, we would like to deform interesting models: for example, animals and
characters. Unlike robots or mechanical objects, the animals tend to deform
smoothly, even near joints: an elbow does not tear into two rigid parts when
bent. Instead, the skin around the elbow stretches and smoothly warps. Skin
closer to the forearm deforms more like the rigid rotation and translation of
the forearm, and likewise the skin near the upper arm deforms like the rigid
upper arm bone. In between, we see a smooth transition or blend.</p>

<p>To approximate this smooth blending quickly on the computer, we begin with a 3D
triangle mesh in its &#8220;rest&#8221; position. The &#8220;rest bones&#8221; are embedded inside of
this model. Each vertex <span class="math">\(i\)</span> of the mesh is assigned a weight <span class="math">\(w_{i,j}\)</span> for each
bone <span class="math">\(j\)</span> corresonding to how much it is &#8220;attached&#8221; to that bone on a scale of 0%
to 100%. Generally, if the rest position of the vertex <span class="math">\(\hat{\v}_i\)</span> is nearer to
a bone <span class="math">\(j\)</span> then its weight <span class="math">\(w_{i,j}\)</span> will be larger. A vertex in the middle of
the elbow may have a weight of 50% for the upper arm and 50% the forearm and
0% for all other bones (toes, fingers, legs, spine, etc.).</p>

<p>Smoothly varying weights produce a smooth deformation. In constrast,
piecewise-constant weights lead to a piece-wise rigid deformation.</p>

<figure>
<img src="images/beast-weights.gif" alt="" />
</figure>

<p>The &#8220;pose&#8221; position <span class="math">\(\v_i\)</span> of this vertex <span class="math">\(i\)</span> will be computed as a weighted
average or linear combination of each bone&#8217;s pose transformation <span class="math">\(\T_j\)</span> applied
to the vertex&#8217;s rest position <span class="math">\(\hat{\v}_i\)</span>:</p>

<p>$$
\v_i =
\sum\limits^{m}_{j=1}
w_{i,j}
\T_j
\left(\begin{array}{c}\hat{\v}_i\\1\end{array}\right).
$$</p>

<blockquote>
<p><strong>Question:</strong> What happens to per-vertex normals after applying a skinning
deformation?</p>

<p><strong>Hint:</strong> 🤯</p>
</blockquote>

<p>Linear blend skinning has many defects. Good &#8220;rigging artists&#8221; can mitigate
these by carefully painting (yes, painting) weight functions and position the
<a href="#2.restbone">rest bones</a>. However, some of the linear blend skinning defects are
theoretical: most notably problems that occur by averaging
rotations as matrices. </p>

<blockquote>
<p><strong>Question:</strong> What transformation matrix do you get if you compute: <span class="math">\(½ \Rot_x(90°) + ½ \Rot_x(-90°)\)</span>?</p>

<p><strong>Hint:</strong> It&#8217;s not <span class="math">\(\Rot_x(0°)\)</span>.</p>
</blockquote>

<h2 id="tasks">Tasks</h2>

<h3 id="whitelist">White List</h3>

<ul>
<li><code>Eigen::Affine3d</code></li>
<li><a href="https://eigen.tuxfamily.org/dox/classEigen_1_1AngleAxis.html"><code>Eigen::AngleAxis</code></a></li>
<li><code>#include &lt;Eigen/Geometry&gt;</code></li>
<li>c++ lambda functions and capturing <code>#include &lt;functional&gt;</code></li>
</ul>

<h3 id="blacklist">Black List</h3>

<ul>
<li><code>igl::lbs</code></li>
<li><code>igl::forward_kinematics</code></li>
</ul>

<h3 id="srceuler_angles_to_transform.cpp"><code>src/euler_angles_to_transform.cpp</code></h3>

<h3 id="srcforward_kinematics.cpp"><code>src/forward_kinematics.cpp</code></h3>

<h3 id="srctransformed_tips.cpp"><code>src/transformed_tips.cpp</code></h3>

<h3 id="srccatmull_rom_interpolation.cpp"><code>src/catmull_rom_interpolation.cpp</code></h3>

<h3 id="srclinear_blend_skinning.cpp"><code>src/linear_blend_skinning.cpp</code></h3>

<h3 id="srccopy_skeleton_at.cpp"><code>src/copy_skeleton_at.cpp</code></h3>

<h3 id="srcend_effectors_objective_and_gradient.cpp"><code>src/end_effectors_objective_and_gradient.cpp</code></h3>

<h3 id="srckinematics_jacobian.cpp"><code>src/kinematics_jacobian.cpp</code></h3>

<h3 id="srcprojected_gradient_descent.cpp"><code>src/projected_gradient_descent.cpp</code></h3>

<h3 id="srcline_search.cpp"><code>src/line_search.cpp</code></h3>

</body>
</html>
